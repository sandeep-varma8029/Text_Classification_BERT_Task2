{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30a9df6-3d94-4a18-b38b-8886f36f15c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.26.114-py3-none-any.whl (135 kB)\n",
      "\u001b[K     |████████████████████████████████| 135 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.30.0,>=1.29.114\n",
      "  Downloading botocore-1.29.114-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 98.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 22.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.114->boto3) (2.8.2)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 114.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.114->boto3) (1.16.0)\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.26.114 botocore-1.29.114 jmespath-1.0.1 s3transfer-0.6.0 urllib3-1.26.15\n"
     ]
    }
   ],
   "source": [
    "#This will install the latest version of the Boto3 library and its dependencies in your Python environment. After running this command, you can import Boto3 and start using it to interact with AWS services in your code.\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509b7b73-54ee-4afc-b9dc-bb1bf12a399f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.11.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: networkx in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.6.1)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6719cc03-74aa-4985-9947-c403434880e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 74.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[K     |████████████████████████████████| 341 kB 92.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[K     |████████████████████████████████| 502 kB 101.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.20.3\n",
      "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 79.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 17.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.3.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\n",
      "\u001b[K     |████████████████████████████████| 768 kB 59.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 95.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 97.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (3.11.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 4.5 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 107.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 110.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: charset-normalizer, certifi, tqdm, requests, tzdata, tokenizers, regex, pytz, numpy, huggingface-hub, transformers, pandas\n",
      "Successfully installed certifi-2022.12.7 charset-normalizer-3.1.0 huggingface-hub-0.13.4 numpy-1.24.2 pandas-2.0.0 pytz-2023.3 regex-2023.3.23 requests-2.28.2 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.28.1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342898d6-2c0e-4d8a-9413-170a5a6e5c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the AWS access key ID and secret access key in the environment variables\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'AKIAZBV6WPTP2UMMX6UO'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'vhYL05sSTrBa2aMY9tKqBK3czdQVrEJ8GPhr7bTj'\n",
    "\n",
    "# Uncomment and use this line if you have a session token\n",
    "# os.environ['AWS_SESSION_TOKEN'] = 'your_session_token'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c5c3a71-6c92-420c-9d8b-1083c804cbd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text language   \n",
      "0  Immigration can have a positive impact on the ...      eng  \\\n",
      "1  Immigration is a complex issue that requires a...      eng   \n",
      "2  The U.S. government should be doing more to pr...      eng   \n",
      "3  Same-sex marriage has been a controversial top...      eng   \n",
      "4  Immigration has been a defining feature of the...      eng   \n",
      "\n",
      "                                label               source  annotator   \n",
      "0                            Economic  The Washington Post  G01352322  \\\n",
      "1  Policy Prescription and Evaluation             The Hill  G01352322   \n",
      "2              Capacity and Resources                  NPR  G01352322   \n",
      "3                    Public Sentiment             ABC News  G01352322   \n",
      "4                   Cultural Identity  National Geographic  G01352322   \n",
      "\n",
      "   label_id  \n",
      "0       1.0  \n",
      "1       6.0  \n",
      "2       2.0  \n",
      "3      12.0  \n",
      "4      11.0  \n"
     ]
    }
   ],
   "source": [
    "# In this code, Boto3 and pandas are used to download a tab-separated file from an S3 bucket and load it into a pandas DataFrame.\n",
    "\n",
    "# First, the necessary libraries are imported: `boto3` is imported to create a client for the S3 service, \n",
    "# `pandas` is imported to load the data into a DataFrame, and `StringIO` is imported to convert the file content from bytes to a string.\n",
    "\n",
    "# A Boto3 client is created for the S3 service using `boto3.client('s3')`. The S3 URI for the file to download is then set, \n",
    "# and the bucket name and file key are extracted using `s3_uri[5:].split('/', 1)`.\n",
    "\n",
    "# The file is downloaded from the S3 bucket using the `get_object` method, \n",
    "# and the content of the file is read and converted to a string using `response['Body'].read().decode('utf-8')`.\n",
    "\n",
    "# The tab-separated data is then loaded into a pandas DataFrame using `pd.read_csv`, \n",
    "# and the first few rows of the DataFrame are printed using `print(aug_df.head())`.\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Create a Boto3 client for the S3 service\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Set the S3 URI for the file to download\n",
    "s3_uri = 's3://sagemaker-studio-r3fybtokjrb/augmented_data.tsv'\n",
    "\n",
    "# Parse the S3 URI to get the bucket name and file key\n",
    "bucket_name, file_key = s3_uri[5:].split('/', 1)\n",
    "\n",
    "# Download the file from the S3 bucket\n",
    "response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "aug_df = pd.read_csv(StringIO(file_content), sep='\\t')  # Use '\\t' for tab-separated files\n",
    "\n",
    "# Print the first few rows of the DataFrame to verify the data was loaded correctly\n",
    "print(aug_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4121a611-fa57-4c75-bb47-3f75d8e3ab98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found device: Tesla T4, n_gpu: 1\n"
     ]
    }
   ],
   "source": [
    "# Import the PyTorch library\n",
    "import torch\n",
    "\n",
    "# Empty the CUDA cache to free up memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Confirm that the GPU is available\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Get the name of the GPU device and the number of available GPUs\n",
    "device_name = torch.cuda.get_device_name()\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# Print the name of the GPU device and the number of available GPUs\n",
    "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
    "\n",
    "# Set the device to the first available GPU\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0586f98c-79c5-49a0-b4e4-68aa0eb40a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89b5db587d34ff2be1ad3e7b9077a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058bc198bacf4c91962d65b5799c4bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1674148bd739438fa50b929e77ca6ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Immigration policies that criminalize or dehumanize immigrants do not reflect our values as a nation of immigrants.\n",
      "Token IDs: tensor([  101, 38451, 35040, 10203, 22078, 22107, 10362, 10102, 31737, 13284,\n",
      "        10732, 41664, 10154, 10497, 63457, 14008, 27749, 10146,   143, 15828,\n",
      "        10108, 41664,   119,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# This code imports a tokenizer helper function from a separate file, preprocesses a DataFrame of text data and labels, \n",
    "# and converts them into tensors that can be used as input to a machine learning model.\n",
    "\n",
    "# Import necessary libraries and helper functions\n",
    "from helpers import tokenize_and_format, flat_accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the DataFrame with augmented data\n",
    "aug_df[\"label_id\"] = aug_df[\"label_id\"].astype(float)\n",
    "df = aug_df\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Extract the text and label columns\n",
    "texts = df.text.values\n",
    "labels = df.label_id.values\n",
    "\n",
    "# Use the tokenizer helper function to convert the text data into input IDs and attention masks\n",
    "input_ids, attention_masks = tokenize_and_format(texts)\n",
    "\n",
    "# Convert the labels to one-hot encoded arrays\n",
    "label_list = []\n",
    "for l in labels:\n",
    "  label_array = np.zeros(len(set(labels)))\n",
    "  label_array[int(l)-1] = 1\n",
    "  label_list.append(label_array)\n",
    "\n",
    "# Convert the input IDs, attention masks, and labels to PyTorch tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(np.array(label_list))\n",
    "\n",
    "# Print the first sentence as text and as a list of input IDs\n",
    "print('Original: ', texts[0])\n",
    "print('Token IDs:', input_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aee5a58-657d-4ca3-8270-0c5e0a7f0bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code splits a preprocessed dataset into training, validation, and testing sets.\n",
    "\n",
    "# Calculate the total number of samples in the dataset\n",
    "total = len(df)\n",
    "\n",
    "# Determine the number of samples to include in each split\n",
    "num_train = int(total * .8)\n",
    "num_val = int(total * .1)\n",
    "num_test = total - num_train - num_val\n",
    "\n",
    "# Make lists of 3-tuples representing each sample in the dataset, with each tuple containing its input IDs, attention masks, and label\n",
    "# The DataFrame was already shuffled in a previous cell\n",
    "train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n",
    "val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]\n",
    "test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]\n",
    "\n",
    "# Make separate lists of the original text data for each split\n",
    "train_text = [texts[i] for i in range(num_train)]\n",
    "val_text = [texts[i] for i in range(num_train, num_val+num_train)]\n",
    "test_text = [texts[i] for i in range(num_val + num_train, total)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f28d29e-11e3-4c7c-a645-044ee3ddc5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code imports a pre-trained BERT model for sequence classification and sets up the model for fine-tuning.\n",
    "\n",
    "# Import necessary libraries and classes from the transformers library\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load a pre-trained BERT model for sequence classification from the transformers library\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-uncased\", # The pre-trained model to load\n",
    "    num_labels = 15, # The number of output labels.\n",
    "    output_attentions = False, # Whether the model returns attention weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden states.\n",
    ")\n",
    "\n",
    "# Uncomment the following line to add a dropout layer to the model\n",
    "# model.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "# Move the model to the GPU for faster training, if available\n",
    "model.cuda()\n",
    "\n",
    "# Print the configuration of the loaded model\n",
    "# print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6c48fca-9420-446f-af44-c3131afc7cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code defines a function to evaluate the performance of the model on a validation set.\n",
    "\n",
    "def get_validation_performance(val_set):\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    # Calculate the number of batches in the validation set\n",
    "    num_batches = int(len(val_set)/batch_size) + 1\n",
    "\n",
    "    total_correct = 0\n",
    "\n",
    "    # Iterate over the batches in the validation set\n",
    "    for i in range(num_batches):\n",
    "      \n",
    "      # Determine the start and end indices of the current batch\n",
    "      end_index = min(batch_size * (i+1), len(val_set))\n",
    "      batch = val_set[i*batch_size:end_index]\n",
    "      \n",
    "      # Skip empty batches\n",
    "      if len(batch) == 0: continue\n",
    "\n",
    "      # Convert the data in the batch to PyTorch tensors\n",
    "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
    "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
    "      label_tensors = torch.stack([data[2] for data in batch])\n",
    "      \n",
    "      # Move tensors to the GPU for faster processing\n",
    "      b_input_ids = input_id_tensors.to(device)\n",
    "      b_input_mask = input_mask_tensors.to(device)\n",
    "      b_labels = label_tensors.to(device)\n",
    "        \n",
    "      # Tell PyTorch not to bother with constructing the compute graph during\n",
    "      # the forward pass, since this is only needed for backprop (training).\n",
    "      with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU for post-processing\n",
    "        logits = (logits).detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the number of correctly labeled examples in the batch\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = np.argmax(label_ids, axis=1).flatten()\n",
    "        num_correct = np.sum(pred_flat == labels_flat)\n",
    "        total_correct += num_correct\n",
    "        \n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"Num of correct predictions =\", total_correct)\n",
    "    avg_val_accuracy = total_correct / len(val_set)\n",
    "    return avg_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe782cab-d10d-4ea5-8c86-fbfe4114797e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "optimizer = AdamW(model.parameters(),lr=5e-05) #with default values of learning rate and epsilon value\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d70c5fd9-0033-4775-84be-6e8a1d12d5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 25 ========\n",
      "Training...\n",
      "Total loss: 10.257633234972571\n",
      "Num of correct predictions = 11\n",
      "Validation accuracy: 0.2682926829268293\n",
      "\n",
      "======== Epoch 2 / 25 ========\n",
      "Training...\n",
      "Total loss: 5.773786653234726\n",
      "Num of correct predictions = 11\n",
      "Validation accuracy: 0.2682926829268293\n",
      "\n",
      "======== Epoch 3 / 25 ========\n",
      "Training...\n",
      "Total loss: 5.028624332927995\n",
      "Num of correct predictions = 11\n",
      "Validation accuracy: 0.2682926829268293\n",
      "\n",
      "======== Epoch 4 / 25 ========\n",
      "Training...\n",
      "Total loss: 4.815865409024991\n",
      "Num of correct predictions = 13\n",
      "Validation accuracy: 0.3170731707317073\n",
      "\n",
      "======== Epoch 5 / 25 ========\n",
      "Training...\n",
      "Total loss: 4.64798665333953\n",
      "Num of correct predictions = 19\n",
      "Validation accuracy: 0.4634146341463415\n",
      "\n",
      "======== Epoch 6 / 25 ========\n",
      "Training...\n",
      "Total loss: 4.326981614035451\n",
      "Num of correct predictions = 16\n",
      "Validation accuracy: 0.3902439024390244\n",
      "\n",
      "======== Epoch 7 / 25 ========\n",
      "Training...\n",
      "Total loss: 3.819434673011096\n",
      "Num of correct predictions = 23\n",
      "Validation accuracy: 0.5609756097560976\n",
      "\n",
      "======== Epoch 8 / 25 ========\n",
      "Training...\n",
      "Total loss: 3.306546515238782\n",
      "Num of correct predictions = 24\n",
      "Validation accuracy: 0.5853658536585366\n",
      "\n",
      "======== Epoch 9 / 25 ========\n",
      "Training...\n",
      "Total loss: 2.7488325724911795\n",
      "Num of correct predictions = 19\n",
      "Validation accuracy: 0.4634146341463415\n",
      "\n",
      "======== Epoch 10 / 25 ========\n",
      "Training...\n",
      "Total loss: 2.3598176896012673\n",
      "Num of correct predictions = 22\n",
      "Validation accuracy: 0.5365853658536586\n",
      "\n",
      "======== Epoch 11 / 25 ========\n",
      "Training...\n",
      "Total loss: 1.9979299532975225\n",
      "Num of correct predictions = 20\n",
      "Validation accuracy: 0.4878048780487805\n",
      "\n",
      "======== Epoch 12 / 25 ========\n",
      "Training...\n",
      "Total loss: 1.72502046151022\n",
      "Num of correct predictions = 23\n",
      "Validation accuracy: 0.5609756097560976\n",
      "\n",
      "======== Epoch 13 / 25 ========\n",
      "Training...\n",
      "Total loss: 1.5457001531069787\n",
      "Num of correct predictions = 22\n",
      "Validation accuracy: 0.5365853658536586\n",
      "\n",
      "======== Epoch 14 / 25 ========\n",
      "Training...\n",
      "Total loss: 1.280129989819446\n",
      "Num of correct predictions = 19\n",
      "Validation accuracy: 0.4634146341463415\n",
      "\n",
      "======== Epoch 15 / 25 ========\n",
      "Training...\n",
      "Total loss: 1.215624936456637\n",
      "Num of correct predictions = 18\n",
      "Validation accuracy: 0.43902439024390244\n",
      "\n",
      "======== Epoch 16 / 25 ========\n",
      "Training...\n",
      "Total loss: 1.0509430078918942\n",
      "Num of correct predictions = 21\n",
      "Validation accuracy: 0.5121951219512195\n",
      "\n",
      "======== Epoch 17 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.8629435176397158\n",
      "Num of correct predictions = 19\n",
      "Validation accuracy: 0.4634146341463415\n",
      "\n",
      "======== Epoch 18 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.7386883704118535\n",
      "Num of correct predictions = 20\n",
      "Validation accuracy: 0.4878048780487805\n",
      "\n",
      "======== Epoch 19 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.6726533646803972\n",
      "Num of correct predictions = 20\n",
      "Validation accuracy: 0.4878048780487805\n",
      "\n",
      "======== Epoch 20 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.6145596877154377\n",
      "Num of correct predictions = 21\n",
      "Validation accuracy: 0.5121951219512195\n",
      "\n",
      "======== Epoch 21 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.5697143467119894\n",
      "Num of correct predictions = 22\n",
      "Validation accuracy: 0.5365853658536586\n",
      "\n",
      "======== Epoch 22 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.5343169325935498\n",
      "Num of correct predictions = 21\n",
      "Validation accuracy: 0.5121951219512195\n",
      "\n",
      "======== Epoch 23 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.4919472815725021\n",
      "Num of correct predictions = 20\n",
      "Validation accuracy: 0.4878048780487805\n",
      "\n",
      "======== Epoch 24 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.4710049582584502\n",
      "Num of correct predictions = 19\n",
      "Validation accuracy: 0.4634146341463415\n",
      "\n",
      "======== Epoch 25 / 25 ========\n",
      "Training...\n",
      "Total loss: 0.4344821330106868\n",
      "Num of correct predictions = 19\n",
      "Validation accuracy: 0.4634146341463415\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# training loop\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    num_batches = int(len(train_set)/batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "      end_index = min(batch_size * (i+1), len(train_set))\n",
    "\n",
    "      batch = train_set[i*batch_size:end_index]\n",
    "\n",
    "      if len(batch) == 0: continue\n",
    "\n",
    "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
    "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
    "      label_tensors = torch.stack([data[2] for data in batch])\n",
    "\n",
    "      # Move tensors to the GPU\n",
    "      b_input_ids = input_id_tensors.to(device)\n",
    "      b_input_mask = input_mask_tensors.to(device)\n",
    "      b_labels = label_tensors.to(device) \n",
    "\n",
    "      # Perform a forward pass (evaluate the model on this training batch).\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "      loss = outputs.loss\n",
    "      logits = outputs.logits\n",
    "\n",
    "      total_train_loss += loss.item()\n",
    "\n",
    "      # Clear the previously calculated gradient\n",
    "      model.zero_grad()     \n",
    "\n",
    "      # Perform a backward pass to calculate the gradients.\n",
    "      loss.backward()\n",
    "\n",
    "      # Update parameters and take a step using the computed gradient.\n",
    "      optimizer.step()\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set. Implement this function in the cell above.\n",
    "    print(f\"Total loss: {total_train_loss}\")\n",
    "    val_acc = get_validation_performance(val_set)\n",
    "    print(f\"Validation accuracy: {val_acc}\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2261d402-a560-46c1-9b2f-b152604a83c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of correct predictions = 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6511627906976745"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_performance(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd0e1d98-ec8a-46bf-a1e0-fc472a4443a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of correct predictions = 28\n",
      "Test accuracy: 0.6511627906976745\n",
      "Number of wrong examples: 15\n",
      "wrong examples:\n",
      "\n",
      "Example 1:\n",
      "Text: Immigration is a polarizing issue that can divide communities.\n",
      "Predicted label: 6\n",
      "True label: 12\n",
      "\n",
      "Example 2:\n",
      "Text: Migration is an opportunity for development economies, but it must be managed in a fair and responsible manner.\n",
      "Predicted label: 2\n",
      "True label: 1\n",
      "\n",
      "Example 3:\n",
      "Text: Same-sex marriage is a significant milestone for the LGBTQ community.\n",
      "Predicted label: 13\n",
      "True label: 11\n",
      "\n",
      "Example 4:\n",
      "Text: Göç politikası, kadınlar ve çocuklar gibi savunmasız nüfusların korunmasına öncelik vermelidir.\n",
      "Predicted label: 4\n",
      "True label: 6\n",
      "\n",
      "Example 5:\n",
      "Text: Eşcinsel evlilik karar vermek için eyaletlere bırakılmamalıdır.\n",
      "Predicted label: 5\n",
      "True label: 13\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_acc = get_validation_performance(test_set)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables\n",
    "total_test_loss = 0\n",
    "total_correct = 0\n",
    "wrong_examples = []\n",
    "\n",
    "num_batches = int(len(test_set)/batch_size) + 1\n",
    "\n",
    "for i in range(num_batches):\n",
    "    end_index = min(batch_size * (i+1), len(test_set))\n",
    "    batch = test_set[i*batch_size:end_index]\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        continue\n",
    "\n",
    "    input_id_tensors = torch.stack([data[0] for data in batch])\n",
    "    input_mask_tensors = torch.stack([data[1] for data in batch])\n",
    "    label_tensors = torch.stack([data[2] for data in batch])\n",
    "\n",
    "    # Move tensors to the GPU\n",
    "    b_input_ids = input_id_tensors.to(device)\n",
    "    b_input_mask = input_mask_tensors.to(device)\n",
    "    b_labels = label_tensors.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Accumulate the test loss\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = (logits).detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the number of correctly labeled examples in batch\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = np.argmax(label_ids, axis=1).flatten()\n",
    "\n",
    "        num_correct = np.sum(pred_flat == labels_flat)\n",
    "        total_correct += num_correct\n",
    "\n",
    "        # Find examples that the model gets wrong\n",
    "        for j in range(len(batch)):\n",
    "            if pred_flat[j] != labels_flat[j]:\n",
    "                text = test_text[i*batch_size+j]\n",
    "                predicted_label = pred_flat[j] + 1\n",
    "                true_label = labels_flat[j] + 1\n",
    "                wrong_examples.append((text, predicted_label, true_label))\n",
    "\n",
    "# Print some of the examples that the model gets wrong\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Number of wrong examples:\", len(wrong_examples))\n",
    "print(\"wrong examples:\")\n",
    "for i, (text, predicted_label, true_label) in enumerate(wrong_examples[:5]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Predicted label:\", predicted_label)\n",
    "    print(\"True label:\", true_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
